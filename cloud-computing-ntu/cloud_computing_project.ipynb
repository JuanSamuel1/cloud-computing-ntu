{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "recreational-identifier",
   "metadata": {},
   "source": [
    "# TextRank Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brutal-values",
   "metadata": {},
   "source": [
    "### Input Text\n",
    "\n",
    "Generated using ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sixth-election",
   "metadata": {},
   "outputs": [],
   "source": [
    "Text = '''TextRank revolutionizes natural language processing with its graph-based approach, providing a powerful means to extract meaningful information from textual data. By modeling text as a graph of interconnected words, TextRank discerns the significance of each word based on its relationships with others. Through iterative ranking iterations, TextRank identifies crucial text units such as keywords and sentences, facilitating tasks like automatic summarization and content recommendation. Its versatility extends beyond simple keyword extraction, offering insights into document clustering, topic modeling, and semantic analysis. With TextRank, the complex landscape of textual information becomes navigable, empowering users to uncover valuable insights with efficiency and precision.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inappropriate-roots",
   "metadata": {},
   "source": [
    "## Text Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "british-landing",
   "metadata": {},
   "source": [
    "The initial text input undergoes a cleaning process to remove any non-printable characters, if present, and is converted to lowercase. Subsequently, the modified text is tokenized using functions provided by the NLTK library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cultural-rates",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Text: \n",
      "\n",
      "['textrank', 'revolutionizes', 'natural', 'language', 'processing', 'with', 'its', 'graph-based', 'approach', ',', 'providing', 'a', 'powerful', 'means', 'to', 'extract', 'meaningful', 'information', 'from', 'textual', 'data', '.', 'by', 'modeling', 'text', 'as', 'a', 'graph', 'of', 'interconnected', 'words', ',', 'textrank', 'discerns', 'the', 'significance', 'of', 'each', 'word', 'based', 'on', 'its', 'relationships', 'with', 'others', '.', 'through', 'iterative', 'ranking', 'iterations', ',', 'textrank', 'identifies', 'crucial', 'text', 'units', 'such', 'as', 'keywords', 'and', 'sentences', ',', 'facilitating', 'tasks', 'like', 'automatic', 'summarization', 'and', 'content', 'recommendation', '.', 'its', 'versatility', 'extends', 'beyond', 'simple', 'keyword', 'extraction', ',', 'offering', 'insights', 'into', 'document', 'clustering', ',', 'topic', 'modeling', ',', 'and', 'semantic', 'analysis', '.', 'with', 'textrank', ',', 'the', 'complex', 'landscape', 'of', 'textual', 'information', 'becomes', 'navigable', ',', 'empowering', 'users', 'to', 'uncover', 'valuable', 'insights', 'with', 'efficiency', 'and', 'precision', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "\n",
    "\n",
    "def clean(text):\n",
    "    text = text.lower()\n",
    "    printable = set(string.printable)\n",
    "    text = filter(lambda x: x in printable, text)\n",
    "    text = \"\".join(list(text))\n",
    "    return text\n",
    "\n",
    "cleaned = clean(Text)\n",
    "text = nltk.word_tokenize(cleaned)\n",
    "\n",
    "print (text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recognized-ultimate",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "The tokenized text, primarily focusing on nouns and adjectives, undergoes normalization through lemmatization. During lemmatization, various grammatical variations of a word are substituted with a single base lemma. For instance, 'ran' might be replaced by 'run'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "buried-wound",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text tokens after lemmatization of adjectives and nouns: \n",
      "\n",
      "['textrank', 'revolutionizes', 'natural', 'language', 'processing', 'with', 'it', 'graph-based', 'approach', ',', 'providing', 'a', 'powerful', 'mean', 'to', 'extract', 'meaningful', 'information', 'from', 'textual', 'data', '.', 'by', 'modeling', 'text', 'a', 'a', 'graph', 'of', 'interconnected', 'word', ',', 'textrank', 'discerns', 'the', 'significance', 'of', 'each', 'word', 'based', 'on', 'it', 'relationship', 'with', 'others', '.', 'through', 'iterative', 'ranking', 'iteration', ',', 'textrank', 'identifies', 'crucial', 'text', 'unit', 'such', 'a', 'keywords', 'and', 'sentence', ',', 'facilitating', 'task', 'like', 'automatic', 'summarization', 'and', 'content', 'recommendation', '.', 'it', 'versatility', 'extends', 'beyond', 'simple', 'keyword', 'extraction', ',', 'offering', 'insight', 'into', 'document', 'clustering', ',', 'topic', 'modeling', ',', 'and', 'semantic', 'analysis', '.', 'with', 'textrank', ',', 'the', 'complex', 'landscape', 'of', 'textual', 'information', 'becomes', 'navigable', ',', 'empowering', 'user', 'to', 'uncover', 'valuable', 'insight', 'with', 'efficiency', 'and', 'precision', '.']\n"
     ]
    }
   ],
   "source": [
    "# nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "POS_tag = nltk.pos_tag(text)\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "adjective_tags = ['JJ','JJR','JJS']\n",
    "\n",
    "lemmatized_text = []\n",
    "\n",
    "for word in POS_tag:\n",
    "    if word[1] in adjective_tags:\n",
    "        lemmatized_text.append(str(wordnet_lemmatizer.lemmatize(word[0],pos=\"a\")))\n",
    "    else:\n",
    "        lemmatized_text.append(str(wordnet_lemmatizer.lemmatize(word[0]))) #default POS = noun\n",
    "        \n",
    "print (\"Text tokens after lemmatization of adjectives and nouns: \\n\")\n",
    "print (lemmatized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adopted-measure",
   "metadata": {},
   "source": [
    "### POS tagging for Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "powerful-consensus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized text with POS tags: \n",
      "\n",
      "[('textrank', 'NN'), ('revolutionizes', 'VBZ'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('with', 'IN'), ('it', 'PRP'), ('graph-based', 'JJ'), ('approach', 'NN'), (',', ','), ('providing', 'VBG'), ('a', 'DT'), ('powerful', 'JJ'), ('mean', 'NN'), ('to', 'TO'), ('extract', 'VB'), ('meaningful', 'JJ'), ('information', 'NN'), ('from', 'IN'), ('textual', 'JJ'), ('data', 'NNS'), ('.', '.'), ('by', 'IN'), ('modeling', 'VBG'), ('text', 'NN'), ('a', 'DT'), ('a', 'DT'), ('graph', 'NN'), ('of', 'IN'), ('interconnected', 'JJ'), ('word', 'NN'), (',', ','), ('textrank', 'NN'), ('discerns', 'VBZ'), ('the', 'DT'), ('significance', 'NN'), ('of', 'IN'), ('each', 'DT'), ('word', 'NN'), ('based', 'VBN'), ('on', 'IN'), ('it', 'PRP'), ('relationship', 'NN'), ('with', 'IN'), ('others', 'NNS'), ('.', '.'), ('through', 'IN'), ('iterative', 'JJ'), ('ranking', 'JJ'), ('iteration', 'NN'), (',', ','), ('textrank', 'JJ'), ('identifies', 'NNS'), ('crucial', 'JJ'), ('text', 'JJ'), ('unit', 'NN'), ('such', 'PDT'), ('a', 'DT'), ('keywords', 'NNS'), ('and', 'CC'), ('sentence', 'NN'), (',', ','), ('facilitating', 'VBG'), ('task', 'NN'), ('like', 'IN'), ('automatic', 'JJ'), ('summarization', 'NN'), ('and', 'CC'), ('content', 'JJ'), ('recommendation', 'NN'), ('.', '.'), ('it', 'PRP'), ('versatility', 'NN'), ('extends', 'VBZ'), ('beyond', 'IN'), ('simple', 'JJ'), ('keyword', 'NN'), ('extraction', 'NN'), (',', ','), ('offering', 'VBG'), ('insight', 'NN'), ('into', 'IN'), ('document', 'NN'), ('clustering', 'NN'), (',', ','), ('topic', 'NN'), ('modeling', 'NN'), (',', ','), ('and', 'CC'), ('semantic', 'JJ'), ('analysis', 'NN'), ('.', '.'), ('with', 'IN'), ('textrank', 'NN'), (',', ','), ('the', 'DT'), ('complex', 'JJ'), ('landscape', 'NN'), ('of', 'IN'), ('textual', 'JJ'), ('information', 'NN'), ('becomes', 'VBZ'), ('navigable', 'JJ'), (',', ','), ('empowering', 'VBG'), ('user', 'NN'), ('to', 'TO'), ('uncover', 'VB'), ('valuable', 'JJ'), ('insight', 'NN'), ('with', 'IN'), ('efficiency', 'NN'), ('and', 'CC'), ('precision', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "POS_tag = nltk.pos_tag(lemmatized_text)\n",
    "\n",
    "print (\"Lemmatized text with POS tags: \\n\")\n",
    "print (POS_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-pharmacy",
   "metadata": {},
   "source": [
    "### POS Based Filtering\n",
    "In this context, any word derived from the lemmatized text that does not belong to the categories of noun, adjective, or gerund (or is identified as a 'foreign word') is regarded as a stopword, indicating non-content. This assumption is rooted in the common observation that keywords typically fall into the categories of nouns, adjectives, or gerunds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "reserved-cincinnati",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = []\n",
    "\n",
    "wanted_POS = ['NN','NNS','NNP','NNPS','JJ','JJR','JJS','VBG','FW'] \n",
    "\n",
    "for word in POS_tag:\n",
    "    if word[1] not in wanted_POS:\n",
    "        stopwords.append(word[0])\n",
    "\n",
    "punctuations = list(str(string.punctuation))\n",
    "\n",
    "stopwords = stopwords + punctuations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accomplished-penny",
   "metadata": {},
   "source": [
    "### Complete stopword generation\n",
    "Stop words are a set of commonly used words in any language. For example, in English, “the”, “is” and “and”, would easily qualify as stop words.\n",
    "\n",
    "(Source of stopwords data: https://www.ranks.nl/stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "liable-sudan",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_file = open(\"long_stopwords.txt\", \"r\")\n",
    "#Source = https://www.ranks.nl/stopwords\n",
    "\n",
    "lots_of_stopwords = []\n",
    "\n",
    "for line in stopword_file.readlines():\n",
    "    lots_of_stopwords.append(str(line.strip()))\n",
    "\n",
    "stopwords_plus = []\n",
    "stopwords_plus = stopwords + lots_of_stopwords\n",
    "stopwords_plus = set(stopwords_plus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expensive-retreat",
   "metadata": {},
   "source": [
    "### Removing Stopwords\n",
    "Removing stopwords into processed text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "oriented-album",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['textrank', 'natural', 'language', 'processing', 'graph-based', 'approach', 'providing', 'powerful', 'meaningful', 'textual', 'data', 'modeling', 'text', 'graph', 'interconnected', 'word', 'textrank', 'significance', 'word', 'relationship', 'iterative', 'ranking', 'iteration', 'textrank', 'identifies', 'crucial', 'text', 'unit', 'keywords', 'sentence', 'facilitating', 'task', 'automatic', 'summarization', 'content', 'recommendation', 'versatility', 'simple', 'keyword', 'extraction', 'offering', 'insight', 'document', 'clustering', 'topic', 'modeling', 'semantic', 'analysis', 'textrank', 'complex', 'landscape', 'textual', 'navigable', 'empowering', 'user', 'valuable', 'insight', 'efficiency', 'precision']\n"
     ]
    }
   ],
   "source": [
    "processed_text = []\n",
    "for word in lemmatized_text:\n",
    "    if word not in stopwords_plus:\n",
    "        processed_text.append(word)\n",
    "print (processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-snowboard",
   "metadata": {},
   "source": [
    "### Vocabulary Creation\n",
    "Vocabulary will only contain unique words from processed_text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "mighty-wilderness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n"
     ]
    }
   ],
   "source": [
    "vocabulary = list(set(processed_text))\n",
    "print (len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-level",
   "metadata": {},
   "source": [
    "### Building Graph\n",
    "TextRank, a graph-based model, constructs a graph where each word in the vocabulary becomes a vertex represented by its index. The weighted_edge matrix signifies edge connections between vertices. \n",
    "\n",
    "If weighted_edge[i][j] is zero, no connection exists between the words indexed by i and j. Words within a specified window in the text are connected. \n",
    "\n",
    "The weight weighted_edge[i][j] increases by 1/(distance between positions of words represented by i and j) for each connection found in different text locations. \n",
    "\n",
    "To prevent redundant counting, covered_cooccurrences keeps track of checked co-occurrences. \n",
    "\n",
    "Vertex scores start at one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "optical-bracelet",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "vocab_len = len(vocabulary)\n",
    "\n",
    "weighted_edge = np.zeros((vocab_len,vocab_len),dtype=np.float32)\n",
    "\n",
    "score = np.zeros((vocab_len),dtype=np.float32)\n",
    "window_size = 3\n",
    "covered_coocurrences = []\n",
    "\n",
    "for i in range(0,vocab_len):\n",
    "    score[i]= 1/vocab_len\n",
    "    for j in range(0,vocab_len):\n",
    "        if j==i:\n",
    "            weighted_edge[i][j]=1\n",
    "        else:\n",
    "            for window_start in range(0,(len(processed_text)-window_size)):\n",
    "                \n",
    "                window_end = window_start+window_size\n",
    "                \n",
    "                window = processed_text[window_start:window_end]\n",
    "                \n",
    "                if (vocabulary[i] in window) and (vocabulary[j] in window):\n",
    "                    \n",
    "                    index_of_i = window_start + window.index(vocabulary[i])\n",
    "                    index_of_j = window_start + window.index(vocabulary[j])\n",
    "                    \n",
    "                    # index_of_x is the absolute position of the xth term in the window \n",
    "                    # (counting from 0) \n",
    "                    # in the processed_text\n",
    "                      \n",
    "                    if [index_of_i,index_of_j] not in covered_coocurrences:\n",
    "                        weighted_edge[i][j]+=1/math.fabs(index_of_i-index_of_j)\n",
    "                        covered_coocurrences.append([index_of_i,index_of_j])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arabic-hampton",
   "metadata": {},
   "source": [
    "### Edge Normalizations\n",
    "The weighted edge is normalized such that the sum of the column equal to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "harmful-treat",
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_edge /= weighted_edge.sum(axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transsexual-tokyo",
   "metadata": {},
   "source": [
    "### Scoring Vertices\n",
    "The Algorithm is run iteratively based on the paper until convergence.\n",
    "\n",
    "d is the damping factor or 1 - teleportation probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "understood-durham",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converging at iteration 3....\n"
     ]
    }
   ],
   "source": [
    "MAX_ITERATIONS = 50\n",
    "d=0.3\n",
    "threshold = 0.0001 #convergence threshold\n",
    "\n",
    "for iter in range(0,MAX_ITERATIONS):\n",
    "    new_score = weighted_edge.dot(score)\n",
    "    new_score = ((1-d) / vocab_len) + (d * new_score)\n",
    "    if np.sum(np.fabs(new_score-score)) <= threshold: #convergence condition\n",
    "        print(\"Converging at iteration \"+str(iter)+\"....\")\n",
    "        break\n",
    "    score = new_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "failing-annotation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of significance: 0.017936839\n",
      "Score of complex: 0.018424816\n",
      "Score of textual: 0.022934457\n",
      "Score of clustering: 0.019026177\n",
      "Score of identifies: 0.018429559\n",
      "Score of automatic: 0.0196074\n",
      "Score of powerful: 0.019290514\n",
      "Score of keywords: 0.019283777\n",
      "Score of precision: 0.019607844\n",
      "Score of language: 0.019316303\n",
      "Score of extraction: 0.019355418\n",
      "Score of modeling: 0.022418316\n",
      "Score of sentence: 0.019556735\n",
      "Score of approach: 0.01959294\n",
      "Score of empowering: 0.019269198\n",
      "Score of graph: 0.018395975\n",
      "Score of task: 0.019605186\n",
      "Score of relationship: 0.018915182\n",
      "Score of summarization: 0.019607844\n",
      "Score of iteration: 0.018716622\n",
      "Score of providing: 0.019557305\n",
      "Score of iterative: 0.019189814\n",
      "Score of text: 0.022634264\n",
      "Score of user: 0.019337704\n",
      "Score of processing: 0.019633736\n",
      "Score of data: 0.01823213\n",
      "Score of unit: 0.018995805\n",
      "Score of facilitating: 0.01959147\n",
      "Score of offering: 0.019143341\n",
      "Score of analysis: 0.01841141\n",
      "Score of meaningful: 0.018996606\n",
      "Score of interconnected: 0.018274061\n",
      "Score of keyword: 0.019567315\n",
      "Score of versatility: 0.019605458\n",
      "Score of navigable: 0.018999556\n",
      "Score of simple: 0.01959433\n",
      "Score of semantic: 0.018539306\n",
      "Score of valuable: 0.019503238\n",
      "Score of natural: 0.018216472\n",
      "Score of recommendation: 0.019607447\n",
      "Score of ranking: 0.019083153\n",
      "Score of topic: 0.018945051\n",
      "Score of insight: 0.023422707\n",
      "Score of crucial: 0.018552803\n",
      "Score of word: 0.021999285\n",
      "Score of efficiency: 0.017658751\n",
      "Score of graph-based: 0.019596724\n",
      "Score of textrank: 0.028551668\n",
      "Score of content: 0.019607844\n",
      "Score of landscape: 0.018566301\n",
      "Score of document: 0.019093879\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,vocab_len):\n",
    "    print(\"Score of \"+vocabulary[i]+\": \"+str(score[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "reported-earthquake",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word\t\tScore\n",
      "-------------------------\n",
      "textrank       \t0.02855167\n",
      "insight        \t0.02342271\n",
      "textual        \t0.02293446\n",
      "text           \t0.02263426\n",
      "modeling       \t0.02241832\n",
      "word           \t0.02199928\n",
      "processing     \t0.01963374\n",
      "precision      \t0.01960784\n",
      "summarization  \t0.01960784\n",
      "content        \t0.01960784\n",
      "recommendation \t0.01960745\n",
      "automatic      \t0.01960740\n",
      "versatility    \t0.01960546\n",
      "task           \t0.01960519\n",
      "graph-based    \t0.01959672\n",
      "simple         \t0.01959433\n",
      "approach       \t0.01959294\n",
      "facilitating   \t0.01959147\n",
      "keyword        \t0.01956731\n",
      "providing      \t0.01955730\n",
      "sentence       \t0.01955673\n",
      "valuable       \t0.01950324\n",
      "extraction     \t0.01935542\n",
      "user           \t0.01933770\n",
      "language       \t0.01931630\n",
      "powerful       \t0.01929051\n",
      "keywords       \t0.01928378\n",
      "empowering     \t0.01926920\n",
      "iterative      \t0.01918981\n",
      "offering       \t0.01914334\n",
      "document       \t0.01909388\n",
      "ranking        \t0.01908315\n",
      "clustering     \t0.01902618\n",
      "navigable      \t0.01899956\n",
      "meaningful     \t0.01899661\n",
      "unit           \t0.01899580\n",
      "topic          \t0.01894505\n",
      "relationship   \t0.01891518\n",
      "iteration      \t0.01871662\n",
      "landscape      \t0.01856630\n",
      "crucial        \t0.01855280\n",
      "semantic       \t0.01853931\n",
      "identifies     \t0.01842956\n",
      "complex        \t0.01842482\n",
      "analysis       \t0.01841141\n",
      "graph          \t0.01839598\n",
      "interconnected \t0.01827406\n",
      "data           \t0.01823213\n",
      "natural        \t0.01821647\n",
      "significance   \t0.01793684\n",
      "efficiency     \t0.01765875\n"
     ]
    }
   ],
   "source": [
    "pairs = [(vocabulary[i], score[i]) for i in range(vocab_len)]\n",
    "pairs.sort(key= lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Word\\t\\tScore\")\n",
    "print(\"-------------------------\")\n",
    "for word, s in pairs:\n",
    "    print(f\"{word.ljust(15)}\\t{s:.8f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleared-christmas",
   "metadata": {},
   "source": [
    "### Phrase Partiotioning\n",
    "Paritioning lemmatized_text into phrases using the stopwords in it as delimeters. The phrases are also candidates for keyphrases to be extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "improving-unknown",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitioned Phrases (Candidate Keyphrases): \n",
      "\n",
      "[['textrank'], ['natural', 'language', 'processing'], ['graph-based', 'approach'], ['providing'], ['powerful'], ['meaningful'], ['textual', 'data'], ['modeling', 'text'], ['graph'], ['interconnected', 'word'], ['textrank'], ['significance'], ['word'], ['relationship'], ['iterative', 'ranking', 'iteration'], ['textrank', 'identifies', 'crucial', 'text', 'unit'], ['keywords'], ['sentence'], ['facilitating', 'task'], ['automatic', 'summarization'], ['content', 'recommendation'], ['versatility'], ['simple', 'keyword', 'extraction'], ['offering', 'insight'], ['document', 'clustering'], ['topic', 'modeling'], ['semantic', 'analysis'], ['textrank'], ['complex', 'landscape'], ['textual'], ['navigable'], ['empowering', 'user'], ['valuable', 'insight'], ['efficiency'], ['precision']]\n"
     ]
    }
   ],
   "source": [
    "phrases = []\n",
    "\n",
    "phrase = \" \"\n",
    "for word in lemmatized_text:\n",
    "    \n",
    "    if word in stopwords_plus:\n",
    "        if phrase!= \" \":\n",
    "            phrases.append(str(phrase).strip().split())\n",
    "        phrase = \" \"\n",
    "    elif word not in stopwords_plus:\n",
    "        phrase+=str(word)\n",
    "        phrase+=\" \"\n",
    "\n",
    "print(\"Partitioned Phrases (Candidate Keyphrases): \\n\")\n",
    "print(phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foster-finnish",
   "metadata": {},
   "source": [
    "### Create a list of unique phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "complimentary-lemon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Phrases (Candidate Keyphrases): \n",
      "\n",
      "[['textrank'], ['natural', 'language', 'processing'], ['graph-based', 'approach'], ['providing'], ['powerful'], ['meaningful'], ['textual', 'data'], ['modeling', 'text'], ['graph'], ['interconnected', 'word'], ['significance'], ['word'], ['relationship'], ['iterative', 'ranking', 'iteration'], ['textrank', 'identifies', 'crucial', 'text', 'unit'], ['keywords'], ['sentence'], ['facilitating', 'task'], ['automatic', 'summarization'], ['content', 'recommendation'], ['versatility'], ['simple', 'keyword', 'extraction'], ['offering', 'insight'], ['document', 'clustering'], ['topic', 'modeling'], ['semantic', 'analysis'], ['complex', 'landscape'], ['textual'], ['navigable'], ['empowering', 'user'], ['valuable', 'insight'], ['efficiency'], ['precision']]\n"
     ]
    }
   ],
   "source": [
    "unique_phrases = []\n",
    "\n",
    "for phrase in phrases:\n",
    "    if phrase not in unique_phrases:\n",
    "        unique_phrases.append(phrase)\n",
    "\n",
    "print(\"Unique Phrases (Candidate Keyphrases): \\n\")\n",
    "print(unique_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opponent-niger",
   "metadata": {},
   "source": [
    "### Further filtering the list of candidate-keyphrases.\n",
    "Removing single word keyphrases-candidates that are present multi-word alternatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "complete-sunset",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thinned Unique Phrases (Candidate Keyphrases): \n",
      "\n",
      "[['natural', 'language', 'processing'], ['graph-based', 'approach'], ['providing'], ['powerful'], ['meaningful'], ['textual', 'data'], ['modeling', 'text'], ['graph'], ['interconnected', 'word'], ['significance'], ['relationship'], ['iterative', 'ranking', 'iteration'], ['textrank', 'identifies', 'crucial', 'text', 'unit'], ['keywords'], ['sentence'], ['facilitating', 'task'], ['automatic', 'summarization'], ['content', 'recommendation'], ['versatility'], ['simple', 'keyword', 'extraction'], ['offering', 'insight'], ['document', 'clustering'], ['topic', 'modeling'], ['semantic', 'analysis'], ['complex', 'landscape'], ['navigable'], ['empowering', 'user'], ['valuable', 'insight'], ['efficiency'], ['precision']]\n"
     ]
    }
   ],
   "source": [
    "for word in vocabulary:\n",
    "    for phrase in unique_phrases:\n",
    "        if (word in phrase) and ([word] in unique_phrases) and (len(phrase)>1):\n",
    "            unique_phrases.remove([word])\n",
    "            \n",
    "print(\"Thinned Unique Phrases (Candidate Keyphrases): \\n\")\n",
    "print(unique_phrases)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grave-mission",
   "metadata": {},
   "source": [
    "### Scoring Keyphrases\n",
    "Scoring the phrases (candidate keyphrases) and building up a list of keyphrases\\keywords by listing untokenized versions of tokenized phrases\\candidate-keyphrases. Phrases are scored by adding the score of their members (words\\text-units that were ranked by the graph algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "latest-netscape",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword: 'natural language processing', Score: 0.057166511192917824\n",
      "Keyword: 'graph-based approach', Score: 0.03918966464698315\n",
      "Keyword: 'providing', Score: 0.019557304680347443\n",
      "Keyword: 'powerful', Score: 0.019290514290332794\n",
      "Keyword: 'meaningful', Score: 0.018996605649590492\n",
      "Keyword: 'textual data', Score: 0.04116658680140972\n",
      "Keyword: 'modeling text', Score: 0.045052580535411835\n",
      "Keyword: 'graph', Score: 0.01839597523212433\n",
      "Keyword: 'interconnected word', Score: 0.04027334600687027\n",
      "Keyword: 'significance', Score: 0.017936838790774345\n",
      "Keyword: 'relationship', Score: 0.01891518197953701\n",
      "Keyword: 'iterative ranking iteration', Score: 0.05698958970606327\n",
      "Keyword: 'textrank identifies crucial text unit', Score: 0.10716409794986248\n",
      "Keyword: 'keywords', Score: 0.019283777102828026\n",
      "Keyword: 'sentence', Score: 0.019556734710931778\n",
      "Keyword: 'facilitating task', Score: 0.03919665515422821\n",
      "Keyword: 'automatic summarization', Score: 0.039215244352817535\n",
      "Keyword: 'content recommendation', Score: 0.039215290918946266\n",
      "Keyword: 'versatility', Score: 0.01960545778274536\n",
      "Keyword: 'simple keyword extraction', Score: 0.05851706303656101\n",
      "Keyword: 'offering insight', Score: 0.042566047981381416\n",
      "Keyword: 'document clustering', Score: 0.03812005557119846\n",
      "Keyword: 'topic modeling', Score: 0.041363367810845375\n",
      "Keyword: 'semantic analysis', Score: 0.03695071488618851\n",
      "Keyword: 'complex landscape', Score: 0.036991117522120476\n",
      "Keyword: 'navigable', Score: 0.018999556079506874\n",
      "Keyword: 'empowering user', Score: 0.038606902584433556\n",
      "Keyword: 'valuable insight', Score: 0.04292594455182552\n",
      "Keyword: 'efficiency', Score: 0.01765875145792961\n",
      "Keyword: 'precision', Score: 0.019607843831181526\n"
     ]
    }
   ],
   "source": [
    "phrase_scores = []\n",
    "keywords = []\n",
    "for phrase in unique_phrases:\n",
    "    phrase_score=0\n",
    "    keyword = ''\n",
    "    for word in phrase:\n",
    "        keyword += str(word)\n",
    "        keyword += \" \"\n",
    "        phrase_score+=score[vocabulary.index(word)]\n",
    "    phrase_scores.append(phrase_score)\n",
    "    keywords.append(keyword.strip())\n",
    "\n",
    "i=0\n",
    "for keyword in keywords:\n",
    "    print (\"Keyword: '\"+str(keyword)+\"', Score: \"+str(phrase_scores[i]))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turkish-connecticut",
   "metadata": {},
   "source": [
    "### Ranking Keyphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "recent-meaning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords:\n",
      "\n",
      "textrank identifies crucial text unit,  simple keyword extraction,  natural language processing,  iterative ranking iteration,  modeling text,  "
     ]
    }
   ],
   "source": [
    "sorted_index = np.flip(np.argsort(phrase_scores),0)\n",
    "\n",
    "keywords_num = 5\n",
    "\n",
    "print(\"Keywords:\\n\")\n",
    "\n",
    "for i in range(0,keywords_num):\n",
    "    print(str(keywords[sorted_index[i]])+\", \", end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attractive-frequency",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
